{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 폴더 구조\n",
    "### 기존 폴더구조\n",
    "```\n",
    " Sample\n",
    " └ 2d_label\n",
    " └ label\n",
    " └ calib\n",
    " └ camera\n",
    " └ lidar\n",
    " └ (points): npy\n",
    " └ (velodyne): bin\n",
    "```\n",
    "    \n",
    "### 변경 폴더구조\n",
    "```\n",
    "custom\n",
    "└ ImageSets\n",
    "└ points\n",
    "└ labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read info of json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/data/NIA50/OpenPCD/data/2-050_sensor_sample\"\n",
    "LABEL = \"2d_label\"\n",
    "label_files = sorted(glob(DATASET_PATH + \"/\" + LABEL + \"/*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 17, 'class': 'Medium_Truck', 'front': [{'x': 543, 'y': 815}, {'x': 587, 'y': 818}, {'x': 585, 'y': 890}, {'x': 541, 'y': 890}], 'back': [{'x': 706, 'y': 802}, {'x': 748, 'y': 806}, {'x': 746, 'y': 886}, {'x': 704, 'y': 886}]}, {'id': 14, 'class': 'Truck', 'front': [{'x': 972, 'y': 811}, {'x': 995, 'y': 817}, {'x': 994, 'y': 898}, {'x': 970, 'y': 898}], 'back': [{'x': 1302, 'y': 892}, {'x': 1303, 'y': 798}, {'x': 1304, 'y': 805}, {'x': 1303, 'y': 893}]}]\n"
     ]
    }
   ],
   "source": [
    "with open(label_files[0], \"r\") as f:\n",
    "    label = json.load(f)\n",
    "    \n",
    "print(label[\"objects\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "categories = set()\n",
    "category_counter = Counter()\n",
    "for label_file in label_files:\n",
    "    with open(label_file, \"r\") as f:\n",
    "        label = json.load(f)\n",
    "    for l in label[\"objects\"]:\n",
    "        categories.add(l['class'])\n",
    "        category_counter[l['class']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Truck', 'truck', 'Medium_Truck', 'bus', 'Bus', 'Car', 'box-svg-selected', 'Adult', 'SUV', 'car'}\n",
      "Counter({'car': 743, 'truck': 435, 'Car': 221, 'Adult': 221, 'bus': 149, 'box-svg-selected': 61, 'Truck': 57, 'SUV': 54, 'Medium_Truck': 53, 'Bus': 52})\n"
     ]
    }
   ],
   "source": [
    "print(categories)\n",
    "print(category_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change cagetory 'car' to 'Car'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lable_file in label_files:\n",
    "    with open(label_file, \"r\") as f:\n",
    "        label = json.load(f)\n",
    "    for l in label[\"objects\"]:\n",
    "        if l['class'] == 'bus':\n",
    "            l['class'] = 'Bus'\n",
    "    \n",
    "    label = json.dumps(label)\n",
    "    with open(os.path.join(DATASET_PATH, LABEL, lable_file), \"w\") as f:\n",
    "        f.write(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pcd to bin\n",
    "# pip3 install --upgrade git+https://github.com/klintan/pypcd.git\n",
    "!python pcd2bin.py --pcd_path /data/NIA50/OpenPCD/data/2-050_sensor_sample/lidar --bin_path /data/NIA50/OpenPCD/data/2-050_sensor_sample/velodyn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Tutorial\n",
    "For the custom dataset template, we only consider the basic scenario: raw point clouds and \n",
    "their corresponding annotations. Point clouds are supposed to be stored in `.npy` format.\n",
    "\n",
    "## Label format\n",
    "We only consider the most basic information -- category and bounding box in the label template.\n",
    "Annotations are stored in the `.txt`. Each line represents a box in a given scene as below:\n",
    "```\n",
    "# format: [x y z dx dy dz heading_angle category_name]\n",
    "1.50 1.46 0.10 5.12 1.85 4.13 1.56 Vehicle\n",
    "5.54 0.57 0.41 1.08 0.74 1.95 1.57 Pedestrian\n",
    "```\n",
    "The box should in the unified 3D box definition (see [README](../README.md))\n",
    "\n",
    "## Files structure\n",
    "Files should be placed as the following folder structure:\n",
    "```\n",
    "OpenPCDet\n",
    "├── data\n",
    "│   ├── custom\n",
    "│   │   │── ImageSets\n",
    "│   │   │   │── train.txt\n",
    "│   │   │   │── val.txt\n",
    "│   │   │── points\n",
    "│   │   │   │── 000000.npy\n",
    "│   │   │   │── 999999.npy\n",
    "│   │   │── labels\n",
    "│   │   │   │── 000000.txt\n",
    "│   │   │   │── 999999.txt\n",
    "├── pcdet\n",
    "├── tools\n",
    "```\n",
    "Dataset splits need to be pre-defined and placed in `ImageSets`\n",
    "\n",
    "## Hyper-parameters Configurations\n",
    "\n",
    "### Point cloud features\n",
    "Modify following configurations in `custom_dataset.yaml` to \n",
    "suit your own point clouds.\n",
    "```yaml\n",
    "POINT_FEATURE_ENCODING: {\n",
    "    encoding_type: absolute_coordinates_encoding,\n",
    "    used_feature_list: ['x', 'y', 'z', 'intensity'],\n",
    "    src_feature_list: ['x', 'y', 'z', 'intensity'],\n",
    "}\n",
    "...\n",
    "# In gt_sampling data augmentation\n",
    "NUM_POINT_FEATURES: 4\n",
    "\n",
    "```\n",
    "\n",
    "#### Point cloud range and voxel sizes\n",
    "For voxel based detectors such as SECOND, PV-RCNN and CenterPoint, the point cloud range and voxel size should follow:\n",
    "1. Point cloud range along z-axis / voxel_size is 40\n",
    "2. Point cloud range along x&y-axis / voxel_size is the multiple of 16.\n",
    "\n",
    "Notice that the second rule also suit pillar based detectors such as PointPillar and CenterPoint-Pillar.\n",
    "\n",
    "### Category names and anchor sizes\n",
    "Category names and anchor size are need to be adapted to custom datasets.\n",
    " ```yaml\n",
    "CLASS_NAMES: ['Vehicle', 'Pedestrian', 'Cyclist']  \n",
    "...\n",
    "MAP_CLASS_TO_KITTI: {\n",
    "    'Vehicle': 'Car',\n",
    "    'Pedestrian': 'Pedestrian',\n",
    "    'Cyclist': 'Cyclist',\n",
    "}\n",
    "...\n",
    "'anchor_sizes': [[3.9, 1.6, 1.56]],\n",
    "...\n",
    "# In gt sampling data augmentation\n",
    "PREPARE: {\n",
    " filter_by_min_points: ['Vehicle:5', 'Pedestrian:5', 'Cyclist:5'],\n",
    " filter_by_difficulty: [-1],\n",
    "}\n",
    "SAMPLE_GROUPS: ['Vehicle:20','Pedestrian:15', 'Cyclist:15']\n",
    "...\n",
    " ```\n",
    "In addition, please also modify the default category names for creating infos in `custom_dataset.py`\n",
    "```\n",
    "create_custom_infos(\n",
    "    dataset_cfg=dataset_cfg,\n",
    "    class_names=['Vehicle', 'Pedestrian', 'Cyclist'],\n",
    "    data_path=ROOT_DIR / 'data' / 'custom',\n",
    "    save_path=ROOT_DIR / 'data' / 'custom',\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Create data info\n",
    "Generate the data infos by running the following command:\n",
    "```shell\n",
    "python -m pcdet.datasets.custom.custom_dataset create_custom_infos tools/cfgs/dataset_configs/custom_dataset.yaml\n",
    "```\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "Here, we only provide an implementation for KITTI stype evaluation.\n",
    "The category mapping between custom dataset and KITTI need to be defined \n",
    "in the `custom_dataset.yaml`\n",
    "```yaml\n",
    "MAP_CLASS_TO_KITTI: {\n",
    "    'Vehicle': 'Car',\n",
    "    'Pedestrian': 'Pedestrian',\n",
    "    'Cyclist': 'Cyclist',\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert 'pcd' file to 'npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypcd import pypcd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "# convert pcd to npy\n",
    "\n",
    "LIDAR_PATH = \"/data/NIA50/OpenPCD/data/2-050_sensor_sample/lidar\"\n",
    "NPY_PATY = \"/data/NIA50/OpenPCD/data/2-050_sensor_sample/points\"\n",
    "\n",
    "lidar_list = sorted(glob(LIDAR_PATH+\"/*.pcd\"))\n",
    "for lidar in lidar_list:\n",
    "    pc = pypcd.PointCloud.from_path(lidar)\n",
    "    points = np.vstack((pc.pc_data['x'], pc.pc_data['y'], pc.pc_data['z'])).transpose()\n",
    "    np.save(NPY_PATY+\"/\"+lidar.split(\"/\")[-1][:-4]+\".npy\", points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create ImageSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1639543832.078959465', '1639543834.178447723', '1639543829.476884127', '1639543828.678121090', '1639543825.876186371', '1639543830.974149466', '1639543827.877243519', '1639543825.978711843', '1639543828.077119112', '1639543834.481409550']\n",
      "['1639543834.076455832', '1639543831.076159239', '1639543832.775555849', '1639543830.277726889', '1639543830.175972700', '1639543829.678906441', '1639543827.978765488', '1639543833.777890205', '1639543826.776987076', '1639543825.777566671']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LIDAR_PATH = \"/data/NIA50/OpenPCD/data/2-050_sensor_sample/lidar\"\n",
    "lidar_list = sorted(glob(LIDAR_PATH+\"/*.pcd\"))\n",
    "\n",
    "filenames = [x[:-4] for x in sorted(os.listdir(LIDAR_PATH))]\n",
    "trainset, valset = train_test_split(filenames, test_size=0.8, random_state=42)\n",
    "print(trainset[:10])\n",
    "print(valset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file names on train.txt and val.txt each line\n",
    "with open(\"/data/NIA50/OpenPCD/data/custom/ImageSets/train.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(trainset))\n",
    "with open(\"/data/NIA50/OpenPCD/data/custom/ImageSets/val.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(valset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Sample 3d Label to Custom Label\n",
    "## Label format\n",
    "\n",
    "label has the most information -- category and bounding box in the label template. Annotations are stored in the `.txt`. Each line represents a box in a given scene as below:\n",
    "\n",
    "```\n",
    "# format: [x y z dx dy dz heading_angle category_name]\n",
    "1.50 1.46 0.10 5.12 1.85 4.13 1.56 Vehicle\n",
    "5.54 0.57 0.41 1.08 0.74 1.95 1.57 Pedestrian\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 422.48it/s]\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from glob import glob\n",
    "from math import radians\n",
    "from tqdm import tqdm\n",
    "LABEL_PATH = \"/data/NIA50/OpenPCD/data/2-050_sensor_sample/label\"\n",
    "LABEL_LIST = sorted(glob(LABEL_PATH+\"/*.json\"))\n",
    "CUSTOM_PATH = \"/data/NIA50/OpenPCD/data/custom/labels\"\n",
    "# {'Truck', 'truck', 'Medium_Truck', 'bus', 'Bus', 'Car', 'box-svg-selected', 'Adult', 'SUV', 'car'}\n",
    "\n",
    "CATEGORY_CONVERT = {'Truck': 'Car', 'truck': 'Car', \n",
    "                    'Medium_Truck': 'Car', 'Van': 'Car',\n",
    "                    'bus': 'Car', 'Bus': 'Car', \n",
    "                    'Car': 'Car', 'box-svg-selected': 'Car', \n",
    "                    'Adult': 'Pedestrian', 'SUV': 'Car', \n",
    "                    'car': 'Car'}\n",
    "\n",
    "\n",
    "for lable in tqdm(LABEL_LIST):\n",
    "    label_info = []\n",
    "    with open(lable, \"r\") as f:\n",
    "        label = json.load(f)\n",
    "    \n",
    "    for lb in label:\n",
    "        x = lb['Value'][0]\n",
    "        y = lb['Value'][1]\n",
    "        z = lb['Value'][2]\n",
    "        xd = lb['y']\n",
    "        yd = lb['x']\n",
    "        zd = lb['z']\n",
    "        heading_angle = radians(lb['Heading'])\n",
    "        category_name = CATEGORY_CONVERT[lb['Category']]\n",
    "        \n",
    "        label_format = f'{x:.4f} {y:.4f} {z:.4f} {xd:.4f} {yd:.4f} {zd:.4f} {heading_angle:.4f} {category_name}'\n",
    "        label_info.append(label_format)\n",
    "    \n",
    "    with open(CUSTOM_PATH + \"/\" + lable.split(\"/\")[-1][:-5] + \".txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(label_info))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'Category': 'Car',\n",
       " 'Heading': -91.43391171464395,\n",
       " 'Type': 'bbox',\n",
       " 'Value': [-0.41179255740374865, 4.658788168227565, -1.3359166681766508],\n",
       " 'x': 2.787668649899378,\n",
       " 'y': 1.6241728810973413,\n",
       " 'z': 0.9390174746513367,\n",
       " 'Attribute': 'none'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0]\n",
    "# x: length, y: width, z: height\n",
    "# Value[0]: y, Value[1]: x, Value[2]: z\n",
    "# Heading: yaw(-pi ~ pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-0.4118 4.6588 -1.3359 2.7877 1.6242 0.9390 -1.5958 Car']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Dataset Infomation\n",
    "\n",
    "`python -m pcdet.datasets.custom.custom_dataset create_custom_infos tools/cfgs/dataset_configs/custom_dataset.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "- To test all the saved checkpoints of a specific training setting and draw the performance curve on the Tensorboard, add the --eval_all argument:<br>\n",
    "`python test.py --cfg_file ${CONFIG_FILE} --batch_size ${BATCH_SIZE} --eval_all`\n",
    "\n",
    "- Test with pretrained model\n",
    "\n",
    "`python test.py --cfg_file ${CONFIG_FILE} --batch_size ${BATCH_SIZE} --ckpt ${CKPT}`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mayavi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1762c5c1f98532ea46af16ba3c4d0b611044a403d445b5fbb2b65112217f0326"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
